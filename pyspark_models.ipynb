{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gcsfs in /opt/conda/anaconda/lib/python3.6/site-packages (0.6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (2.22.0)\n",
      "Requirement already satisfied: google-auth>=1.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (1.11.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (4.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (0.4.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (0.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (1.14.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (4.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (45.2.0.post20200210)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/anaconda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n",
      "python version:  3.6.10\n",
      "CPU times: user 127 ms, sys: 15.5 ms, total: 143 ms\n",
      "Wall time: 1.59 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-66ee-m.asia-south1-a.c.weicheng.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "!pip install gcsfs\n",
    "import pandas as pd\n",
    "import os, sys, time, json, re, string, datetime\n",
    "\n",
    "import gcsfs\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, StorageLevel, keyword_only\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.param.shared import HasInputCol, HasInputCols, HasOutputCol, HasOutputCols, Param\n",
    "from pyspark.ml.feature import OneHotEncoder, HashingTF, IDF, Tokenizer, RegexTokenizer, NGram, CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover, VectorAssembler, PCA, OneHotEncoderEstimator,StringIndexer\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, DecisionTreeClassifier, RandomForestClassifier, LinearSVC\n",
    "from pyspark.ml.classification import GBTClassifier, MultilayerPerceptronClassifier\n",
    "\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from platform import python_version\n",
    "print(\"python version: \", python_version())\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.32 ms, sys: 0 ns, total: 7.32 ms\n",
      "Wall time: 8.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"A0186371Y\") \\\n",
    "        .config(\"spark.master\", \"yarn\") \\\n",
    "        .config(\"spark.submit.deployMode\", \"cluster\") \\\n",
    "        .config(\"spark.driver.memory\", \"10g\") \\\n",
    "        .config(\"spark.driver.cores\", \"4\") \\\n",
    "        .config(\"spark.executor.instances\", \"3\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44183 entries, 0 to 44182\n",
      "Data columns (total 3 columns):\n",
      "Comment    44183 non-null object\n",
      "Outcome    44183 non-null int64\n",
      "Id         44183 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.0+ MB\n",
      "CPU times: user 689 ms, sys: 80.1 ms, total: 769 ms\n",
      "Wall time: 2.17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    24992\n",
       "0    19191\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pd_df_train = pd.read_csv('gs://dataproc-6ca41800-27b4-47d5-abee-55c011dfa389-asia-southeast1/data/kaggle/train.csv')\n",
    "pd_df_test = pd.read_csv('gs://dataproc-6ca41800-27b4-47d5-abee-55c011dfa389-asia-southeast1/data/kaggle/test.csv')\n",
    "pd_df_train.info()\n",
    "pd_df_train['Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Comment: string (nullable = true)\n",
      " |-- Outcome: integer (nullable = true)\n",
      " |-- Id: string (nullable = true)\n",
      "\n",
      "+-------+-----+\n",
      "|Outcome|count|\n",
      "+-------+-----+\n",
      "|      1|24992|\n",
      "|      0|19191|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "py_df_train = spark.createDataFrame(pd_df_train, schema = \"Comment STRING, Outcome INT, Id STRING\")\n",
    "py_df_test = spark.createDataFrame(pd_df_test, schema = \"Comment STRING, Id STRING\")\n",
    "\n",
    "py_df_train.printSchema()\n",
    "py_df_train.groupby('Outcome').count().show()\n",
    "\n",
    "data_train, data_test = py_df_train.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized transformer class to manually extract some counting based word features\n",
    "class TextFeatureTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=\"Comment\", outputCol=\"word_features\"):\n",
    "        super(TextFeatureTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)        \n",
    "\n",
    "    def _transform(self, dataset):        \n",
    "        def f(data):\n",
    "            word_count = len(data)\n",
    "            unique_word_count = len(set(data))\n",
    "            char_count = 0 \n",
    "            dict_punct_count = {}                      \n",
    "            for c in data:\n",
    "                if str(c) in string.punctuation:\n",
    "                    dict_punct_count[str(c)] = dict_punct_count.get(str(c), 0) + 1                \n",
    "                else: \n",
    "                    char_count += 1\n",
    "\n",
    "            punctuation_count = sum(dict_punct_count.values())        \n",
    "            feature_list = [word_count,\n",
    "                            char_count, char_count / (word_count + 1e-10),\n",
    "                            unique_word_count, unique_word_count / (word_count + 1e-10),\n",
    "                            punctuation_count, punctuation_count / (word_count + 1e-10)]\n",
    "\n",
    "            puntuation_list = [\"!\", \"?\", \"=\", \"{\", \"<\", \">\", \"(\", \"+\", \"-\", \"*\", \"/\", \"[\"]             \n",
    "            for p in puntuation_list:\n",
    "                value = 0.0\n",
    "                for key in dict_punct_count:\n",
    "                    if key == p:\n",
    "                        value = dict_punct_count.get(key, 0)\n",
    "\n",
    "                feature_list.append(value)\n",
    "                feature_list.append(value / (word_count + 1e-10))\n",
    "            \n",
    "            return Vectors.dense(feature_list)\n",
    "        \n",
    "        return dataset.withColumn(self.getOutputCol(), F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(vocab_size=5000):\n",
    "    preproc_steps = [\n",
    "        RegexTokenizer(inputCol=\"Comment\", outputCol=\"all_words\", pattern=r\"\\W\"),\n",
    "        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n",
    "        CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=vocab_size),\n",
    "        IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\"),\n",
    "        TextFeatureTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n",
    "        VectorAssembler(inputCols=[\"tfidf_features\", \"word_features\"], outputCol=\"features\")\n",
    "    ]\n",
    "    return Pipeline(stages=preproc_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models(df_train, df_test, labelCol = 'Outcome', featuresCol='features'):\n",
    "    lr_model = LogisticRegression(featuresCol=featuresCol, labelCol=labelCol,\n",
    "                                  predictionCol='prediction', probabilityCol='probability', \n",
    "                                  rawPredictionCol='rawPrediction',\n",
    "                                  family='binomial', fitIntercept=True, \n",
    "                                  threshold=0.5, standardization=False, \n",
    "                                  maxIter=200, regParam=0.001, elasticNetParam=0, tol=1e-06,  aggregationDepth=2)\n",
    "    \n",
    "    lr_param_grid = ParamGridBuilder() \\\n",
    "                    .addGrid(lr_model.regParam, [0.001, 0.005]) \\\n",
    "                    .addGrid(lr_model.standardization, [False, True]) \\\n",
    "                    .addGrid(lr_model.elasticNetParam, [0.0, 0.5 , 1]) \\\n",
    "                    .build()\n",
    "    \n",
    "    svm_model = LinearSVC(featuresCol=featuresCol, labelCol=labelCol, \n",
    "                          predictionCol='prediction', rawPredictionCol='rawPrediction', \n",
    "                          maxIter=100, regParam=0.001, tol=1e-06,\n",
    "                          fitIntercept=True, standardization=False, threshold=0.0, aggregationDepth=2)\n",
    "    \n",
    "    svm_param_grid = ParamGridBuilder() \\\n",
    "                        .addGrid(svm_model.regParam, [0.01, 0.005]) \\\n",
    "                        .addGrid(svm_model.standardization, [False, True]) \\\n",
    "                        .build()\n",
    "    \n",
    "    dt_model = DecisionTreeClassifier(featuresCol=featuresCol, labelCol=labelCol,\n",
    "                                      predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', \n",
    "                                      maxDepth=10, maxBins=32, \n",
    "                                      minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=2048, \n",
    "                                      cacheNodeIds=True, checkpointInterval=10, impurity='gini', seed=666) \n",
    "    \n",
    "    dt_param_grid = ParamGridBuilder() \\\n",
    "                     .addGrid(dt_model.maxDepth, [10, 20, 30])  \\\n",
    "                     .addGrid(dt_model.maxBins, [20, 40, 60])  \\\n",
    "                     .build()\n",
    "        \n",
    "    rf_model = RandomForestClassifier(featuresCol=featuresCol, labelCol=labelCol,\n",
    "                                      predictionCol='prediction', probabilityCol='probability', \n",
    "                                      rawPredictionCol='rawPrediction',\n",
    "                                      maxDepth=10, maxBins=32, numTrees=200,\n",
    "                                      minInstancesPerNode=1, minInfoGain=0.0, \n",
    "                                      maxMemoryInMB=2048, cacheNodeIds=True, \n",
    "                                      checkpointInterval=10, impurity='gini', \n",
    "                                      featureSubsetStrategy='auto', seed=666, subsamplingRate=0.8)\n",
    "    rf_param_grid = ParamGridBuilder() \\\n",
    "                     .addGrid(rf_model.maxDepth, [10, 20, 30])  \\\n",
    "                     .addGrid(rf_model.maxBins, [20, 40, 60])  \\\n",
    "                     .addGrid(rf_model.numTrees, [100, 200, 300])  \\\n",
    "                     .build()\n",
    "    \n",
    "    gbt_model = GBTClassifier(featuresCol=featuresCol, labelCol=labelCol, maxIter=250)\n",
    "    gbt_param_grid = ParamGridBuilder().build()\n",
    "\n",
    "    mp_model = MultilayerPerceptronClassifier(featuresCol=featuresCol, labelCol=labelCol,\n",
    "                                              predictionCol='prediction', \n",
    "                                              layers=[df_train.schema[\"features\"].metadata[\"ml_attr\"][\"num_attrs\"],20, 10, 2],  \n",
    "                                              maxIter=250, blockSize=128, seed=1234)\n",
    "    gbt_param_grid = ParamGridBuilder().build()\n",
    "    \n",
    "    nb_model = NaiveBayes(featuresCol='features', labelCol='Outcome', \n",
    "                          predictionCol='prediction', probabilityCol='probability',\n",
    "                          rawPredictionCol='rawPrediction',\n",
    "                          smoothing=1, modelType='multinomial')\n",
    "    nb_param_grid = ParamGridBuilder().build()\n",
    "    \n",
    "    ##[model_name, model, parameter_gride, run_flag, tuning_flag, probability_col]\n",
    "    models = [[\"LogisticRegression\", lr_model, lr_param_grid, True, False, \"probability\"],\n",
    "              [\"LinearSVC\",svm_model, svm_param_grid, True, False, \"rawPrediction\"],\n",
    "              [\"DecisionTreeClassifier\",dt_model, dt_param_grid, True, False, \"probability\"],\n",
    "              [\"RandomForestClassifier\",rf_model, rf_param_grid, True, False, \"probability\"],\n",
    "              [\"GBTClassifier\", gbt_model, gbt_param_grid, True, False, \"probability\"],\n",
    "              [\"MultilayerPerceptronClassifier\" , mp_model, gbt_param_grid, True, False, \"probability\"],\n",
    "              [\"NaiveBayes\",nb_model, nb_param_grid, True, False, \"probability\"]]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(models, df_train, df_test, label_col=\"Outcome\", featuresCol='features', prediction_col=\"prediction\"): \n",
    "    best_model_name = \"\"\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    for item in models:        \n",
    "        name = item[0]\n",
    "        model = item[1]\n",
    "        param_grid = item[2]\n",
    "        run_flag = item[3]\n",
    "        tuning_flag = item[4]\n",
    "        probability_col = item[5]\n",
    "        \n",
    "        if run_flag:                \n",
    "            print(\"**********Run Model {0}**********\".format(name))    \n",
    "            t = time.time()        \n",
    "            selected_model = model\n",
    "\n",
    "            if tuning_flag:\n",
    "                evaluator = BinaryClassificationEvaluator(rawPredictionCol=probability_col, labelCol=label_col, metricName=\"areaUnderROC\")\n",
    "                crossval = CrossValidator(estimator=model, evaluator=evaluator,estimatorParamMaps=param_grid,\n",
    "                                          numFolds=2, parallelism=3, seed=234)\n",
    "                crossval_model = crossval.fit(df_train)\n",
    "                selected_model = crossval_model.bestModel \n",
    "                print(\"***Best Params: \") \n",
    "                print(selected_model.explainParams())\n",
    "            else:\n",
    "                selected_model = model.fit(df_train)\n",
    "\n",
    "            pred_dataset = selected_model.transform(df_test)\n",
    "\n",
    "            eval_dataset = pred_dataset.select(label_col, prediction_col, probability_col)\n",
    "            model_eval = MulticlassClassificationEvaluator(predictionCol=prediction_col, labelCol=label_col)\n",
    "            accuracy_val = model_eval.evaluate(eval_dataset, {model_eval.metricName: 'accuracy'})\n",
    "            print(\"accuracy = {0}\".format(accuracy_val))    \n",
    "            print(\"time taken for {0}: {1}\".format(name, str(datetime.timedelta(seconds=time.time() - t))))\n",
    "\n",
    "        if accuracy_val > best_accuracy:\n",
    "            best_model_name = name\n",
    "            best_model = selected_model\n",
    "            best_accuracy = accuracy_val\n",
    "    \n",
    "    return best_model_name, best_model, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 174 ms, sys: 27.8 ms, total: 201 ms\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab_size = 2000\n",
    "preproc_model = build_pipeline(vocab_size).fit(data_train)\n",
    "df_train = preproc_model.transform(data_train).select(\"Outcome\", \"features\")\n",
    "df_test = preproc_model.transform(data_test).select(\"Outcome\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Outcome=0, features=SparseVector(2031, {0: 1.8053, 2: 2.3572, 4: 2.9163, 8: 3.6794, 24: 2.085, 30: 1.7174, 33: 3.817, 42: 2.4743, 62: 2.8825, 64: 7.7216, 89: 2.6441, 130: 3.3884, 171: 3.7089, 191: 3.4429, 192: 3.509, 222: 6.5558, 282: 3.4106, 294: 4.2749, 354: 4.0608, 444: 4.1202, 799: 4.8562, 1174: 11.0987, 1768: 15.9829, 1823: 5.5493, 2000: 50.0, 2001: 50.0, 2002: 1.0, 2003: 36.0, 2004: 0.72}))]\n"
     ]
    }
   ],
   "source": [
    "print(df_train.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Outcome=1, features=SparseVector(2031, {0: 0.9026, 6: 1.6535, 18: 2.0475, 24: 6.2549, 41: 1.9133, 47: 5.1899, 135: 2.711, 145: 3.1581, 164: 2.9046, 283: 3.591, 338: 4.4729, 572: 4.4055, 601: 4.5558, 844: 4.779, 1322: 4.9241, 1471: 5.5865, 1477: 5.2764, 2000: 23.0, 2001: 23.0, 2002: 1.0, 2003: 19.0, 2004: 0.8261}))]\n"
     ]
    }
   ],
   "source": [
    "print(df_test.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Run Model LogisticRegression**********\n",
      "accuracy = 0.6778942672317645\n",
      "time taken for LogisticRegression: 0:00:41.824251\n",
      "**********Run Model LinearSVC**********\n",
      "accuracy = 0.6792326567031006\n",
      "time taken for LinearSVC: 0:09:50.264606\n",
      "**********Run Model DecisionTreeClassifier**********\n",
      "accuracy = 0.6143207673432969\n",
      "time taken for DecisionTreeClassifier: 0:00:16.515173\n",
      "**********Run Model RandomForestClassifier**********\n",
      "accuracy = 0.6359580637965648\n",
      "time taken for RandomForestClassifier: 0:00:29.678597\n",
      "**********Run Model GBTClassifier**********\n",
      "accuracy = 0.6868168637073389\n",
      "time taken for GBTClassifier: 0:12:41.413851\n",
      "**********Run Model MultilayerPerceptronClassifier**********\n",
      "accuracy = 0.67945572161499\n",
      "time taken for MultilayerPerceptronClassifier: 0:01:28.141524\n",
      "**********Run Model NaiveBayes**********\n",
      "accuracy = 0.6272585322328798\n",
      "time taken for NaiveBayes: 0:00:05.084994\n",
      "**********Winner is GBTClassifier with accuracy = 0.6868168637073389**********\n",
      "CPU times: user 1.55 s, sys: 587 ms, total: 2.14 s\n",
      "Wall time: 25min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "models = build_models(df_train, df_test, labelCol='Outcome', featuresCol='features')\n",
    "best_model_name, best_model, best_accuracy = run_models(models, df_train, df_test)\n",
    "print(\"**********Winner is {0} with accuracy = {1}**********\".format(best_model_name, best_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
